{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c47b10",
   "metadata": {},
   "source": [
    "# Multi-Agent Customer Service System\n",
    " \n",
    "## Overview\n",
    "This notebook demonstrates how to build a **multi-agent system** using Semantic Kernel where different specialized agents handle specific types of customer inquiries.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"lesson_1.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</div>\n",
    " \n",
    "### Key Concepts Covered:\n",
    "1. **Agent Specialization**: Each agent has a specific domain of expertise\n",
    "2. **Boundary Enforcement**: Agents decline requests outside their domain\n",
    "3. **Service Configuration**: Setting up Azure OpenAI with custom parameters\n",
    "4. **Asynchronous Processing**: Using async/await for efficient agent communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1acf2c6",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51eecd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "BASE_URL = os.getenv(\"URL\")\n",
    "API_VERSION = \"2024-12-01-preview\"\n",
    "DEPLOYMENT = \"gpt-4.1-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef6ec5",
   "metadata": {},
   "source": [
    "## 2. Initialize Kernel and AI Service\n",
    " \n",
    "The **Kernel** is the core orchestration layer that manages services and plugins.\n",
    "#We configure an Azure OpenAI chat service and register it with the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe50e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel()\n",
    "\n",
    "# Create Azure OpenAI chat service\n",
    "chat_service = AzureChatCompletion(\n",
    "    deployment_name=DEPLOYMENT,\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    base_url=f\"{BASE_URL}{DEPLOYMENT}\",\n",
    "    api_version=API_VERSION\n",
    ")\n",
    "\n",
    "# Register the service with the kernel\n",
    "kernel.add_service(chat_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a94869",
   "metadata": {},
   "source": [
    "## 3. Agent Factory Function\n",
    " \n",
    "This helper function creates agents with customized settings:\n",
    "- **temperature**: Controls response randomness (0.0 = deterministic, 1.0 = creative)\n",
    "- **max_tokens**: Limits response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a37008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(name: str, instructions: str, temperature=0.7, max_tokens=400):\n",
    "    \"\"\"\n",
    "    Creates a ChatCompletionAgent with specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        name: Agent identifier\n",
    "        instructions: System prompt defining agent's role and boundaries\n",
    "        temperature: Response creativity (default: 0.7)\n",
    "        max_tokens: Maximum response length (default: 400)\n",
    "    \n",
    "    Returns:\n",
    "        ChatCompletionAgent instance\n",
    "    \"\"\"\n",
    "    settings = OpenAIChatPromptExecutionSettings()\n",
    "    settings.temperature = temperature\n",
    "    settings.max_tokens = max_tokens\n",
    "    \n",
    "    return ChatCompletionAgent(\n",
    "        service=chat_service,\n",
    "        name=name,\n",
    "        instructions=instructions,\n",
    "        arguments=KernelArguments(settings)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b20a6",
   "metadata": {},
   "source": [
    "## 4. Define Specialized Agents\n",
    " \n",
    " Each agent is an expert in a specific customer service domain.\n",
    " Notice how each instruction explicitly defines:\n",
    " - What the agent **can** handle\n",
    " - What to do when encountering requests **outside** their expertise\n",
    "\n",
    "### 4.1 Billing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58073c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "billing_agent = create_agent(\n",
    "    \"BillingAgent\",\n",
    "    \"\"\"\n",
    "You are a billing and payment specialist for a retail company.\n",
    "If the question is about invoices, payments, refunds, charges, or billing issues, provide helpful answers.\n",
    "If the question is NOT related to billing or payments, respond with: \"I cannot help with this. Please contact the appropriate department.\"\n",
    "\"\"\",\n",
    "    temperature=0.5  # Lower temperature for consistent financial information\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea15918",
   "metadata": {},
   "source": [
    "### 4.2 Technical Support Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c94e696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_support_agent = create_agent(\n",
    "    \"TechSupportAgent\",\n",
    "    \"\"\"\n",
    "You are a technical support specialist.\n",
    "If the question is about product troubleshooting, technical issues, setup, or how to use features, provide assistance.\n",
    "If the question is NOT technical in nature, respond with: \"I cannot help with this. Please contact the appropriate department.\"\n",
    "\"\"\",\n",
    "    temperature=0.6  # Slightly creative for problem-solving\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df7466",
   "metadata": {},
   "source": [
    "### 4.3 Product Information Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffdcca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_info_agent = create_agent(\n",
    "    \"ProductInfoAgent\",\n",
    "    \"\"\"\n",
    "You are a product information specialist.\n",
    "If the question is about product specifications, availability, features, or comparisons, answer it.\n",
    "If the question is NOT about product information, respond with: \"I cannot help with this. Please contact the appropriate department.\"\n",
    "\"\"\",\n",
    "    temperature=0.7  # Balanced for informative responses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c3af2",
   "metadata": {},
   "source": [
    "### 4.4 Returns and Exchanges Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e5f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_agent = create_agent(\n",
    "    \"ReturnsAgent\",\n",
    "    \"\"\"\n",
    "You are a returns and exchanges specialist.\n",
    "If the question is about return policies, exchange procedures, shipping returns, or product complaints, help the customer.\n",
    "If the question is NOT about returns or exchanges, respond with: \"I cannot help with this. Please contact the appropriate department.\"\n",
    "\"\"\",\n",
    "    temperature=0.5  # Consistent policy information\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8a5ba8",
   "metadata": {},
   "source": [
    "## 5. Collect all agents in a list for iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55548055",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [billing_agent, tech_support_agent, product_info_agent, returns_agent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113376ce",
   "metadata": {},
   "source": [
    "## 6. Run Customer Inquiries\n",
    " \n",
    "We'll test various customer questions to see how each agent responds.\n",
    "**Expected behavior**: Only the relevant agent provides a full answer; others decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d4f1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"\n",
    "    Simulates customer inquiries being routed to all agents.\n",
    "    In production, you'd implement smart routing to send queries only to relevant agents.\n",
    "    \"\"\"\n",
    "    \n",
    "    customer_inquiries = [\n",
    "        \"Why was I charged twice on my last order?\",\n",
    "        \"My laptop won't turn on after the latest update. What should I do?\",\n",
    "        \"Do you have the wireless headphones in blue color?\",\n",
    "        \"I want to return a defective product I bought last week\",\n",
    "        \"What are the specs of your newest smartphone model?\",\n",
    "        \"How can I track my refund status?\"\n",
    "    ]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CUSTOMER SERVICE MULTI-AGENT ROUTING SYSTEM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, inquiry in enumerate(customer_inquiries, start=1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Customer Inquiry #{i}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Question: {inquiry}\\n\")\n",
    "        \n",
    "        # Route inquiry to all agents (in production, use smart routing)\n",
    "        for agent in agents:\n",
    "            print(f\"[{agent.name}]\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Invoke agent asynchronously and stream response\n",
    "            async for message in agent.invoke(inquiry):\n",
    "                print(f\"{message}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e7a91",
   "metadata": {},
   "source": [
    "## 7. Run the Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b9cacde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUSTOMER SERVICE MULTI-AGENT ROUTING SYSTEM\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Customer Inquiry #1\n",
      "============================================================\n",
      "Question: Why was I charged twice on my last order?\n",
      "\n",
      "[BillingAgent]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ServiceResponseException",
     "evalue": "(\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", NotFoundError(\"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:88\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     87\u001b[39m         settings_dict.pop(\u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(**settings_dict)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2672\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2671\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2673\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2674\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2675\u001b[39m         {\n\u001b[32m   2676\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2677\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2678\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2679\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2680\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2681\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2682\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2683\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2684\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2685\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2686\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2687\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2688\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2689\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2690\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2691\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2692\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2693\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2694\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2697\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2698\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2699\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2700\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2701\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2702\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2703\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2704\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2705\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2706\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2707\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2708\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2709\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2710\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2711\u001b[39m         },\n\u001b[32m   2712\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2713\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2714\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2715\u001b[39m     ),\n\u001b[32m   2716\u001b[39m     options=make_request_options(\n\u001b[32m   2717\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2718\u001b[39m     ),\n\u001b[32m   2719\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2720\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2721\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2722\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1791\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1593\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceResponseException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Invoke agent asynchronously and stream response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m agent.invoke(inquiry):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/semantic_kernel/utils/telemetry/agent_diagnostics/decorators.py:106\u001b[39m, in \u001b[36mtrace_agent_invocation.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(invoke_func)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_decorator\u001b[39m(\n\u001b[32m    101\u001b[39m     *args: P.args,\n\u001b[32m    102\u001b[39m     **kwargs: P.kwargs,\n\u001b[32m    103\u001b[39m ) -> AsyncIterable[AgentResponseItem[ChatMessageContent]]:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[32m    105\u001b[39m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the responses\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m invoke_func(*args, **kwargs):\n\u001b[32m    107\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m response\n\u001b[32m    108\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/semantic_kernel/agents/chat_completion/chat_completion_agent.py:364\u001b[39m, in \u001b[36mChatCompletionAgent.invoke\u001b[39m\u001b[34m(self, messages, thread, on_intermediate_message, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m thread.get_messages():\n\u001b[32m    362\u001b[39m     chat_history.add_message(message)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_invoke(\n\u001b[32m    365\u001b[39m     thread,\n\u001b[32m    366\u001b[39m     chat_history,\n\u001b[32m    367\u001b[39m     on_intermediate_message,\n\u001b[32m    368\u001b[39m     arguments,\n\u001b[32m    369\u001b[39m     kernel,\n\u001b[32m    370\u001b[39m     **kwargs,\n\u001b[32m    371\u001b[39m ):\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m AgentResponseItem(message=response, thread=thread)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/semantic_kernel/agents/chat_completion/chat_completion_agent.py:537\u001b[39m, in \u001b[36mChatCompletionAgent._inner_invoke\u001b[39m\u001b[34m(self, thread, history, on_intermediate_message, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m    533\u001b[39m message_count_before_completion = \u001b[38;5;28mlen\u001b[39m(agent_chat_history)\n\u001b[32m    535\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Invoking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chat_completion_service).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m responses = \u001b[38;5;28;01mawait\u001b[39;00m chat_completion_service.get_chat_message_contents(\n\u001b[32m    538\u001b[39m     chat_history=agent_chat_history,\n\u001b[32m    539\u001b[39m     settings=settings,\n\u001b[32m    540\u001b[39m     kernel=kernel,\n\u001b[32m    541\u001b[39m     arguments=arguments,\n\u001b[32m    542\u001b[39m )\n\u001b[32m    544\u001b[39m logger.debug(\n\u001b[32m    545\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Invoked \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chat_completion_service).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    546\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith message count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_count_before_completion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    547\u001b[39m )\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m# Drain newly added tool messages since last index to maintain\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# correct order and avoid duplicates\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/semantic_kernel/connectors/ai/chat_completion_client_base.py:139\u001b[39m, in \u001b[36mChatCompletionClientBase.get_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m use_span(\u001b[38;5;28mself\u001b[39m._start_auto_function_invocation_activity(kernel, settings), end_on_exit=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m _:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m request_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(settings.function_choice_behavior.maximum_auto_invoke_attempts):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m         completions = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_get_chat_message_contents(chat_history, settings)\n\u001b[32m    140\u001b[39m         \u001b[38;5;66;03m# Get the function call contents from the chat message. There is only one chat message,\u001b[39;00m\n\u001b[32m    141\u001b[39m         \u001b[38;5;66;03m# which should be checked in the `_verify_function_choice_settings` method.\u001b[39;00m\n\u001b[32m    142\u001b[39m         function_calls = [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m completions[\u001b[32m0\u001b[39m].items \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, FunctionCallContent)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/semantic_kernel/utils/telemetry/model_diagnostics/decorators.py:112\u001b[39m, in \u001b[36mtrace_chat_completion.<locals>.inner_trace_chat_completion.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(completion_func)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_decorator\u001b[39m(*args: Any, **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[ChatMessageContent]:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[32m    111\u001b[39m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m completion_func(*args, **kwargs)\n\u001b[32m    114\u001b[39m     completion_service: \u001b[33m\"\u001b[39m\u001b[33mChatCompletionClientBase\u001b[39m\u001b[33m\"\u001b[39m = args[\u001b[32m0\u001b[39m]\n\u001b[32m    115\u001b[39m     chat_history: ChatHistory = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_chat_completion_base.py:88\u001b[39m, in \u001b[36mOpenAIChatCompletionBase._inner_get_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings)\u001b[39m\n\u001b[32m     85\u001b[39m settings.messages = \u001b[38;5;28mself\u001b[39m._prepare_chat_history_for_request(chat_history)\n\u001b[32m     86\u001b[39m settings.ai_model_id = settings.ai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_id\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_request(settings)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[32m     90\u001b[39m response_metadata = \u001b[38;5;28mself\u001b[39m._get_metadata_from_chat_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:60\u001b[39m, in \u001b[36mOpenAIHandler._send_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.TEXT \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.CHAT:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_completion_request(settings)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.EMBEDDING:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIEmbeddingPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/agentic/cd14705-azure-agentic-c2-classroom/.venv/lib/python3.13/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:105\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    102\u001b[39m         ex,\n\u001b[32m    103\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    106\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    107\u001b[39m         ex,\n\u001b[32m    108\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mServiceResponseException\u001b[39m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", NotFoundError(\"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\"))"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda3518",
   "metadata": {},
   "source": [
    " ## Key Takeaways\n",
    " \n",
    " ### 1. **Agent Specialization**\n",
    " Each agent has a narrow, well-defined role. This mirrors real-world customer service departments.\n",
    " \n",
    " ### 2. **Boundary Enforcement**\n",
    " Agents explicitly decline out-of-scope requests, preventing hallucinations and maintaining reliability.\n",
    " \n",
    " ### 3. **Scalability**\n",
    " This pattern scales easily: add new agents (e.g., \"ShippingAgent\") by following the same template.\n",
    " \n",
    " ### 4. **Production Considerations**\n",
    " - **Smart Routing**: Use a classifier agent or intent detection to route to the right agent first\n",
    " - **Fallback Agent**: Add a general agent for questions no specialist can handle\n",
    " - **Logging**: Track which agents handle which queries for analytics\n",
    " - **Human Escalation**: Some queries should trigger human agent involvement\n",
    " \n",
    " ### 5. **Alternative Patterns**\n",
    " - **Sequential Processing**: Route through agents in order until one handles it\n",
    " - **Voting System**: Multiple agents respond; best answer is selected\n",
    " - **Hierarchical**: Supervisor agent delegates to specialist agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cd14705-azure-agentic-c2-classroom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
